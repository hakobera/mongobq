#!/usr/bin/env node

var program = require('commander');
var MongoBQ = require('../lib/mongobq');
var pkg = require('../package');
var path = require('path');
var fs = require('fs');

program
  .version(pkg.version)
  .option('--host <hostname>', 'specifies a hostname for the mongod. (Default: "localhost")')
  .option('--port <port>', 'specifies the TCP port for the mongod. (Default: 27017)', parseInt)
  .option('-u, --username <username>', 'specifies a mongodb username to authenticate')
  .option('-p, --password <password>', 'specifies a mongodb password to authenticate')
  .option('-d, --db <database>', 'specifies the name of the database')
  .option('-c, --collection <collection>', 'specifies the collection to export')
  .option('-f, --fields <field1[,field2]>', 'specifies a field or fields to include in the export')
  .option('-q, --query <JSON>', 'provides a JSON document as a query that optionally limits the documents returned in the export', JSON.parse)
  .option('-K, --keyfile <keyfile>', 'specifies the key file path')
  .option('-B, --bucket <bucket>', 'specifies the Google Cloud Storage bucket name')
  .option('-O, --path <path>', 'specifies the output path of the bucket (Default: "/")')
  .option('-P, --project <project>', 'specifies the project ID of Google Cloud Platform')
  .option('-D, --dataset <dataset>', 'specifies the dataset ID of BigQuery')
  .option('-T, --table <table>', 'specifies the table ID of BigQuery')
  .option('-S, --schema <schemafile>', 'specifies the table schema of BigQuery table to import', parseSchema)
  .option('--autoclean', 'clean after run')
  .option('--async', 'no wait load job')
  .option('--nocompress', 'nocompress data when upload to Google Cloud Storage')
  .option('--dryrun', 'run as dryrun mode')
  .parse(process.argv);

var opts = {
  host: program.host || 'localhost',
  port: program.port || 27017,
  database: program.db,
  collection: program.collection,
  username: program.username,
  password: program.password,
  query: program.query || {},
  fields: parseFields(program.schema, program.fields),
  project: program.project,
  keyfile: program.keyfile,
  bucket: program.bucket,
  directory: program.directory,
  dataset: program.dataset,
  table: program.table,
  schema: program.schema,
  autoclean: program.autoclean,
  async: program.async,
  compress: !program.nocompress,
  dryrun: program.dryrun
};

if (checkArgs(opts)) {
  new MongoBQ(opts).run();
} else {
  program.help();
}

function checkArgs(args) {
  if (!required(args, 'database')) return false;
  if (!required(args, 'collection')) return false;
  if (!required(args, 'project')) return false;
  if (!required(args, 'bucket')) return false;
  if (!required(args, 'dataset')) return false;

  return true;
}

function required(args, key) {
  if (!args[key]) {
    console.error(key + ' must specified.');
    return false;
  }
  return true;
}

function parseFields(schema, fields) {
  if (schema) {
    return extractFields(schema);
  } else if (fields) {
    return fields.split(',');
  } else {
    return [];
  }
}

function parseSchema(schemaFile) {
  try {
    return JSON.parse(fs.readFileSync(schemaFile, { encoding: 'utf8' }));
  } catch (e) {
    console.error(e);
    process.exit(1);
  }
}

function extractFields(schema) {
  return schema.map(function (f) {
    return f.name;
  }).filter(function (f) {
    return f !== 'id';
  });
}
